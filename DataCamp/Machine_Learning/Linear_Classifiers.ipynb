{"cells":[{"source":"## Linear Classifiers \nIn this course you'll learn all about using linear classifiers, specifically `logistic regression` and `support vector machines`, with `scikit-learn`. Once you've learned how to apply these methods, you'll dive into the ideas behind them and find out what really makes them tick. At the end of this course you'll know how to train, test, and tune these linear classifiers in Python. You'll also have a conceptual foundation for understanding many other machine learning algorithms.","metadata":{},"cell_type":"markdown","id":"b352a04a-21b8-4527-8ec4-ef722469d3e2"},{"source":"### Applying logistic regression and SVM\nIn this chapter you will learn the basics of applying logistic regression and support vector machines (SVMs) to classification problems. You'll use the scikit-learn library to fit classification models to real data.\n\n#### KNN Classification\nIn this exercise you'll explore a subset of the Large Movie Review Dataset. The variables X_train, X_test, y_train, and y_test are already loaded into the environment. The X variables contain features based on the words in the movie reviews, and the y variables contain labels for whether the review sentiment is positive (+1) or negative (-1).\n\n* pre-loaded variables that have captured the dataset split into a training/test split\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create and fit the model\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\n# Predict on the test features, print the results\npred = knn.predict(X_test)[0]\nprint(\"Prediction for test example 0:\", pred)\n\n# Logistic Type Value output\n<script.py> output:\n    Prediction for test example 0: 1.0\n```\n\n#### Comparing models\nCompare k nearest neighbors classifiers with k=1 and k=5 on the handwritten digits data set, which is already loaded into the variables X_train, y_train, X_test, and y_test. You can set k with the n_neighbors parameter when creating the KNeighborsClassifier object, which is also already imported into the environment.\n\nWhich model has a higher test accuracy?\n\n* knn.score(test_x, y_test) returns the mean accuracy on the given test data and labels\n\n```python\nIn [1]:\nknn = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\nIn [2]:\nknn.score(X_test, y_test)\nOut[2]:\n0.9888888888888889\nIn [3]:\nknn = KNeighborsClassifier(5).fit(X_train, y_train)\nIn [4]:\nknn.score(X_test, y_test)\nOut[4]:\n0.9933333333333333\nIn [5]:\nknn = KNeighborsClassifier(5).fit(X_train, y_train)\nIn [6]:\nknn.predict(X_test)\nOut[6]:\n\narray([1, 5, 0, 7, 1, 0, 6, 1, 5, 4, 9, 2, 7, 8, 4, 6, 9, 3, 7, 4, 7, 1,\n       8, 6, 0, 9, 6, 1, 3, 7, 5, 9, 8, 3, 2, 8, 8, 1, 1, 0, 7, 9, 0, 0,\n       8, 7, 2, 7, 4, 3, 4, 3, 4, 0, 4, 7, 0, 5, 5, 5, 2, 1, 7, 0, 5, 1,\n       8, 3, 3, 4, 0, 3, 7, 4, 3, 4, 2, 9, 7, 3, 2, 5, 3, 4, 1, 5, 5, 2,\n       5, 2, 2, 2, 2, 7, 0, 8, 1, 7, 4, 2, 3, 8, 2, 3, 3, 0, 2, 9, 9, 2,\n       3, 2, 8, 1, 1, 9, 1, 2, 0, 4, 8, 5, 4, 4, 7, 6, 7, 6, 6, 1, 7, 5,\n       6, 3, 8, 3, 7, 1, 8, 5, 3, 4, 7, 8, 5, 0, 6, 0, 6, 3, 7, 6, 5, 6,\n       2, 2, 2, 3, 0, 7, 6, 5, 6, 4, 1, 0, 6, 0, 6, 4, 0, 9, 3, 8, 1, 2,\n       3, 1, 9, 0, 7, 6, 2, 9, 3, 5, 3, 4, 6, 3, 3, 7, 4, 9, 2, 7, 6, 1,\n       6, 8, 4, 0, 3, 1, 0, 9, 9, 9, 0, 1, 8, 6, 8, 0, 9, 5, 9, 8, 2, 3,\n       5, 3, 0, 8, 7, 4, 0, 3, 3, 3, 6, 3, 3, 2, 9, 1, 6, 9, 0, 4, 2, 2,\n       7, 9, 1, 6, 7, 6, 3, 9, 1, 9, 3, 4, 0, 6, 4, 8, 5, 3, 6, 3, 1, 4,\n       0, 4, 4, 8, 7, 9, 1, 5, 2, 7, 0, 9, 0, 4, 4, 0, 1, 0, 6, 4, 2, 8,\n       5, 0, 2, 6, 0, 1, 8, 2, 0, 9, 5, 6, 7, 0, 5, 0, 9, 1, 4, 7, 1, 7,\n       0, 6, 6, 8, 0, 2, 2, 6, 9, 9, 7, 5, 1, 7, 6, 4, 6, 1, 9, 4, 7, 1,\n       3, 7, 8, 1, 6, 9, 8, 3, 2, 4, 8, 7, 5, 5, 6, 9, 9, 8, 5, 0, 0, 4,\n       9, 3, 0, 4, 9, 4, 2, 5, 4, 9, 6, 4, 2, 6, 0, 0, 5, 6, 7, 1, 9, 2,\n       5, 1, 5, 9, 8, 7, 7, 0, 6, 9, 3, 1, 9, 3, 9, 8, 7, 0, 2, 3, 9, 9,\n       2, 8, 1, 9, 3, 3, 0, 0, 7, 3, 8, 7, 9, 9, 7, 1, 0, 4, 5, 4, 1, 7,\n       3, 6, 5, 4, 9, 0, 5, 9, 1, 4, 5, 0, 4, 3, 4, 2, 3, 9, 0, 8, 7, 8,\n       6, 9, 4, 5, 7, 8, 3, 7, 8, 3])\nIn [7]:\nknn.score(X_test, y_test)\nOut[7]:\n0.9933333333333333\n```\n* Doesn't appear the model's accuracy is in need of predictions for the test labels prior to scoring the model accuracy\n\n\n#### Overfitting : Check For Understanding\nWhich of the following situations looks like an example of overfitting?\n1. Training accuracy 50%, testing accuracy 50%.\n2. Training accuracy 95%, testing accuracy 95%.\n3. **Training accuracy 95%, testing accuracy 50%**\n4. Training accuracy 50%, testing accuracy 95%.\n* High training accuracy suggests the model was `overfit` with features that measured the training data well but didn't capture the test data well and thus was likely overfit with features in the model creation : overcompex model \n* Inverse is true for `underfit` in which the training score may be lower than the test score : too simple of a model ","metadata":{},"cell_type":"markdown","id":"870d268a-da4f-4eeb-a5b2-994f4b83bd83"},{"source":"### Applying logistic regression and SVM\n#### \"predict_proba\" - Understanding\n* scikit-learn's LogisticRegression can also output confidence scores rather than \"hard\" or definite predictions.\n* Let's do this with the `\"predict_proba\"` function and test it out on the first training example.\n    * Sample Lr model using wine dataset from sklearn.datasets : `wine.data` is the defined features\n    * Predict on first training example : `lr.predict_proba(wine.data[:1])`\n        *  returns : array([[9.966e-01, 2.740e-03, 6.787e-04]])\n        *  first probability as 9-point-9 times 10 to the power of -1, or point-99, or 99%\n        *  https://calculator.name/scientific-notation-to-decimal/9.966e-01\n","metadata":{},"cell_type":"markdown","id":"de3bf617-71d5-4bc5-adbc-5ea8d453e841"},{"source":"[float(x) for x in [9.966e-01, 2.740e-03, 6.787e-04]]","metadata":{"executionTime":73,"lastSuccessfullyExecutedCode":"[float(x) for x in [9.966e-01, 2.740e-03, 6.787e-04]]"},"cell_type":"code","id":"3082b8a4-8bb0-4b64-ba11-0ea4d82f1b4f","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"[0.9966, 0.00274, 0.0006787]"},"metadata":{}}]},{"source":"### Running LogisticRegression and SVC\nIn this exercise, you'll apply logistic regression and a support vector machine to classify images of handwritten digit","metadata":{},"cell_type":"markdown","id":"fae28e2e-ca53-418f-85fa-8e4ece1e0c07"},{"source":"from sklearn import datasets\ndigits = datasets.load_digits()\nprint(type(digits), type(digits.data), type(digits.target), digits.data.shape, digits.target.shape)","metadata":{"executionTime":30,"lastSuccessfullyExecutedCode":"from sklearn import datasets\ndigits = datasets.load_digits()\nprint(type(digits), type(digits.data), type(digits.target), digits.data.shape, digits.target.shape)"},"cell_type":"code","id":"01549077-538d-4bd1-b65a-75bb75147f08","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'sklearn.utils._bunch.Bunch'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> (1797, 64) (1797,)\n"}]},{"source":"# For each classifier, print out the training and validation accuracy.\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n\n# Apply logistic regression and print scores\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(lr.score(X_train, y_train))\nprint(lr.score(X_test, y_test))\n\nprint('\\n')\n# Apply SVM and print scores\nsvm = SVC()\nsvm.fit(X_train, y_train)\nprint(svm.score(X_train, y_train))\nprint(svm.score(X_test, y_test))","metadata":{"executionTime":814,"lastSuccessfullyExecutedCode":"# For each classifier, print out the training and validation accuracy.\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n\n# Apply logistic regression and print scores\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(lr.score(X_train, y_train))\nprint(lr.score(X_test, y_test))\n\nprint('\\n')\n# Apply SVM and print scores\nsvm = SVC()\nsvm.fit(X_train, y_train)\nprint(svm.score(X_train, y_train))\nprint(svm.score(X_test, y_test))"},"cell_type":"code","id":"15b72576-8e79-48b4-bfcb-a846aad58973","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"1.0\n0.9555555555555556\n\n\n0.9977728285077951\n0.98\n"}]},{"source":"### Sentiment analysis for movie reviews\nIn this exercise you'll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset.\n\nThe variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1).\n\n```python\n# Instantiate logistic regression and train\nlr = LogisticRegression()\nlr.fit(X, y)\n\n# Predict sentiment for a glowing review\nreview1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n\n# Predict sentiment for a poor review\nreview2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\nreview2_features = get_features(review2)\nprint(\"Review:\", review2)\nprint(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])\n\nReview: LOVED IT! This movie was amazing. Top 10 this year.\nProbability of positive review: 0.8111238392808809\nReview: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\nProbability of positive review: 0.5888052699327708\n```\n* The second probability would have been even lower, but the word \"good\" trips it up a bit, since that's considered a \"positive\" word.","metadata":{},"cell_type":"markdown","id":"193fe983-fa11-48b5-b0ce-f8ea9af8bf79"},{"source":"### Visualizing decision boundaries\nIn this exercise, you'll visualize the decision boundaries of various classifier types.","metadata":{},"cell_type":"markdown","id":"c0332925-c2bc-4316-8833-9e533e3ce5bb"},{"source":"from sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_wine\n\nwine = load_wine()\nprint(type(wine.data), type(wine.target), wine.target.shape, wine.data.shape)\nX = wine.data \ny = wine.target ","metadata":{"executionTime":109,"lastSuccessfullyExecutedCode":"from sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_wine\n\nwine = load_wine()\nprint(type(wine.data), type(wine.target), wine.target.shape, wine.data.shape)\nX = wine.data \ny = wine.target "},"cell_type":"code","id":"6571853a-b11b-446c-88f6-652abce0dce0","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'numpy.ndarray'> <class 'numpy.ndarray'> (178,) (178, 13)\n"}]},{"source":"* pre-loaded function in exercise and using the inspect module to get the source of the function\n```python\nIn [3]:\nimport inspect\nIn [4]:\nlines = inspect.getsource(plot_4_classifiers)\nIn [5]:\nprint(lines)\ndef plot_4_classifiers(X, y, clfs):\n\n    # Set-up 2x2 grid for plotting.\n    fig, sub = plt.subplots(2, 2)\n    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n\n    for clf, ax, title in zip(clfs, sub.flatten(), (\"(1)\", \"(2)\", \"(3)\", \"(4)\")):\n        # clf.fit(X, y)\n        plot_classifier(X, y, clf, ax, ticks=True)\n        ax.set_title(title)\n    plt.show()\n```","metadata":{},"cell_type":"markdown","id":"380b66a4-8711-4eef-a6b2-c1a7a42d1c43"},{"source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_contours(ax, clf, xx, yy, proba=False, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    if proba:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,-1]\n        Z = Z.reshape(xx.shape)\n        out = ax.imshow(Z,extent=(np.min(xx), np.max(xx), np.min(yy), np.max(yy)), origin='lower', vmin=0, vmax=1, **params)\n        ax.contour(xx, yy, Z, levels=[0.5])\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        out = ax.contourf(xx, yy, Z, **params)\n    return out\n\ndef make_meshgrid(x, y, h=.02, lims=None):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n\n    if lims is None:\n        x_min, x_max = x.min() - 1, x.max() + 1\n        y_min, y_max = y.min() - 1, y.max() + 1\n    else:\n        x_min, x_max, y_min, y_max = lims\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n    \ndef plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None): # assumes classifier \"clf\" is already fit\n    X0, X1 = X[:, 0], X[:, 1]\n    xx, yy = make_meshgrid(X0, X1, lims=lims)\n\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n        show = True\n    else:\n        show = False\n\n    # can abstract some of this into a higher-level function for learners to call\n    cs = plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8, proba=proba)\n    if proba:\n        cbar = plt.colorbar(cs)\n        cbar.ax.set_ylabel('probability of red $\\Delta$ class', fontsize=20, rotation=270, labelpad=30)\n        cbar.ax.tick_params(labelsize=14)\n    #ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors='k', linewidth=1)\n    labels = np.unique(y)\n    if len(labels) == 2:\n        ax.scatter(X0[y==labels[0]], X1[y==labels[0]], cmap=plt.cm.coolwarm, s=60, c='b', marker='o', edgecolors='k')\n        ax.scatter(X0[y==labels[1]], X1[y==labels[1]], cmap=plt.cm.coolwarm, s=60, c='r', marker='^', edgecolors='k')\n    else:\n        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k', linewidth=1)\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n#     ax.set_xlabel(data.feature_names[0])\n#     ax.set_ylabel(data.feature_names[1])\n    if ticks:\n        ax.set_xticks(())\n        ax.set_yticks(())\n#     ax.set_title(title)\n    if show:\n        plt.show()\n    else:\n        return ax\n    \ndef plot_4_classifiers(X, y, clfs):\n\n    # Set-up 2x2 grid for plotting.\n    fig, sub = plt.subplots(2, 2)\n    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n\n    for clf, ax, title in zip(clfs, sub.flatten(), (\"(1)\", \"(2)\", \"(3)\", \"(4)\")):\n        # clf.fit(X, y)\n        plot_classifier(X, y, clf, ax, ticks=True)\n        ax.set_title(title)\n    plt.show ","metadata":{"executionTime":77,"lastSuccessfullyExecutedCode":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_contours(ax, clf, xx, yy, proba=False, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    if proba:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,-1]\n        Z = Z.reshape(xx.shape)\n        out = ax.imshow(Z,extent=(np.min(xx), np.max(xx), np.min(yy), np.max(yy)), origin='lower', vmin=0, vmax=1, **params)\n        ax.contour(xx, yy, Z, levels=[0.5])\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        out = ax.contourf(xx, yy, Z, **params)\n    return out\n\ndef make_meshgrid(x, y, h=.02, lims=None):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n\n    if lims is None:\n        x_min, x_max = x.min() - 1, x.max() + 1\n        y_min, y_max = y.min() - 1, y.max() + 1\n    else:\n        x_min, x_max, y_min, y_max = lims\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n    \ndef plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None): # assumes classifier \"clf\" is already fit\n    X0, X1 = X[:, 0], X[:, 1]\n    xx, yy = make_meshgrid(X0, X1, lims=lims)\n\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n        show = True\n    else:\n        show = False\n\n    # can abstract some of this into a higher-level function for learners to call\n    cs = plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8, proba=proba)\n    if proba:\n        cbar = plt.colorbar(cs)\n        cbar.ax.set_ylabel('probability of red $\\Delta$ class', fontsize=20, rotation=270, labelpad=30)\n        cbar.ax.tick_params(labelsize=14)\n    #ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors='k', linewidth=1)\n    labels = np.unique(y)\n    if len(labels) == 2:\n        ax.scatter(X0[y==labels[0]], X1[y==labels[0]], cmap=plt.cm.coolwarm, s=60, c='b', marker='o', edgecolors='k')\n        ax.scatter(X0[y==labels[1]], X1[y==labels[1]], cmap=plt.cm.coolwarm, s=60, c='r', marker='^', edgecolors='k')\n    else:\n        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k', linewidth=1)\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n#     ax.set_xlabel(data.feature_names[0])\n#     ax.set_ylabel(data.feature_names[1])\n    if ticks:\n        ax.set_xticks(())\n        ax.set_yticks(())\n#     ax.set_title(title)\n    if show:\n        plt.show()\n    else:\n        return ax\n    \ndef plot_4_classifiers(X, y, clfs):\n\n    # Set-up 2x2 grid for plotting.\n    fig, sub = plt.subplots(2, 2)\n    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n\n    for clf, ax, title in zip(clfs, sub.flatten(), (\"(1)\", \"(2)\", \"(3)\", \"(4)\")):\n        # clf.fit(X, y)\n        plot_classifier(X, y, clf, ax, ticks=True)\n        ax.set_title(title)\n    plt.show "},"cell_type":"code","id":"08a621ec-26bf-4e8a-a526-f411cc113a69","execution_count":25,"outputs":[]},{"source":"# Create the following classifier objects with default hyperparameters: LogisticRegression, LinearSVC, SVC, KNeighborsClassifier\nclassifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n\n# Fit each of the classifiers on the provided data using a for loop. (using target and data for wine_data declared with dataset import)\nfor c in classifiers:\n    c.fit(X, y)\n    \n# plot_4_classifiers(X, y, classifiers)\n# plt.show()","metadata":{"executionTime":66,"lastSuccessfullyExecutedCode":"# Create the following classifier objects with default hyperparameters: LogisticRegression, LinearSVC, SVC, KNeighborsClassifier\nclassifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n\n# Fit each of the classifiers on the provided data using a for loop. (using target and data for wine_data declared with dataset import)\nfor c in classifiers:\n    c.fit(X, y)\n    \n# plot_4_classifiers(X, y, classifiers)\n# plt.show()"},"cell_type":"code","id":"85b45a03-dc39-4630-8e5e-86475cd1162e","execution_count":27,"outputs":[]},{"source":"* Arguments aren't lining up with the defined functions in the exercise\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Define the classifiers\nclassifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n\n# Fit each of the classifiers on the provided data using a for loop. (using target and data for wine_data declared with dataset import)\nfor c in classifiers:\n    c.fit(X, y)\n    \nplot_4_classifiers(X, y, classifiers)\nplt.show()\n```\n![Screen Shot 2023-03-21 at 11.32.15 AM](Screen%20Shot%202023-03-21%20at%2011.32.15%20AM.png)\n\n* As you can see, `logistic regression` and `linear SVM` are linear classifiers whereas `KNN` is not. The default `SVM` is also non-linear, but this is hard to see in the plot because it performs poorly with default hyperparameters. With better hyperparameters, it performs well.\n    * Matplotlib has subplots of 2 columns and 2 rows first row has a linear decision boundary, second classifiers (models) are not linear with their default hyperparameters return ","metadata":{},"cell_type":"markdown","id":"539fb1f2-b282-41ef-b7fb-0d187f4111dc"},{"source":"## Loss functions\n* In this chapter you will discover the conceptual framework behind logistic regression and SVMs. This will let you delve deeper into the inner workings of these models.\n* This chapter is much more conceptual than the other chapters, because we'll be laying the foundation for understanding logistic regression and SVMs.\n* We'll start off by exploring some math behind linear classifiers in this video.","metadata":{},"cell_type":"markdown","id":"7a347c8f-42b1-495e-8241-61e5c6a91323"},{"source":"# Dot Products\n## We'll start by defining a dot product. Let's create some numpy arrays x and y. To take the dot product between them, we need to multiply them element-wise.\n\nx = np.arange(3)\ny = np.arange(3, 6)\nprint(x,y)\nprint(x * y) # product of values at same list index\nprint(np.sum(x*y)) # sum values of product multiplication return above\nprint(x@y) # x@y is the dot product of x and y, and is written as x * y","metadata":{"executionTime":78,"lastSuccessfullyExecutedCode":"# Dot Products\n## We'll start by defining a dot product. Let's create some numpy arrays x and y. To take the dot product between them, we need to multiply them element-wise.\n\nx = np.arange(3)\ny = np.arange(3, 6)\nprint(x,y)\nprint(x * y) # product of values at same list index\nprint(np.sum(x*y)) # sum values of product multiplication return above\nprint(x@y) # x@y is the dot product of x and y, and is written as x * y"},"cell_type":"code","id":"0716fdf0-8b95-4024-9291-650e04660fbb","execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":"[0 1 2] [3 4 5]\n[ 0  4 10]\n14\n14\n"}]},{"source":"* You can think of a `dot product` as **multiplication in higher dimensions**, since `x` and `y` are arrays of values.","metadata":{},"cell_type":"markdown","id":"0c308324-3797-43bf-844b-5e7bcdb82339"},{"source":"### Changing the model coefficients\nWhen you call fit with scikit-learn, the logistic regression coefficients are automatically learned from your dataset. In this exercise you will explore how the decision boundary is represented by the `coefficients`. To do so, you will change the coefficients manually (instead of with fit), and visualize the resulting classifiers.","metadata":{},"cell_type":"markdown","id":"3fb1a412-4d5d-4082-9fdb-abd281bcd7aa"},{"source":"model = LogisticRegression()\nX = np.array([[ 1.78862847,  0.43650985],[ 0.09649747, -1.8634927 ],[-0.2773882 , -0.35475898],[-3.08274148,  2.37299932],\n[-3.04381817,  2.52278197],[-1.31386475,  0.88462238],[-2.11868196,  4.70957306],[-2.94996636,  2.59532259],\n[-3.54535995,  1.45352268],[ 0.98236743, -1.10106763],[-1.18504653, -0.2056499 ],[-1.51385164,  3.23671627],\n[-4.02378514,  2.2870068 ],[ 0.62524497, -0.16051336],[-3.76883635,  2.76996928],[ 0.74505627,  1.97611078],[-1.24412333, -0.62641691],[-0.80376609, -2.41908317],[-0.92379202, -1.02387576],[ 1.12397796, -0.13191423]])\ny = np.array([-1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1, -1])\n\n# Set the two coefficients and the intercept to various values and observe the resulting decision boundaries.\n# Set the coefficients\nmodel.coef_ = np.array([[-1,1]])\nmodel.intercept_ = np.array([-3])\n\n# Plot the data and decision boundary\n#plot_classifier(X,y,model)\n\n# Print the number of errors\nnum_err = np.sum(y != model.predict(X))\nprint(\"Number of errors:\", num_err)","metadata":{"executionTime":98,"lastSuccessfullyExecutedCode":"model = LogisticRegression()\nX = np.array([[ 1.78862847,  0.43650985],[ 0.09649747, -1.8634927 ],[-0.2773882 , -0.35475898],[-3.08274148,  2.37299932],\n[-3.04381817,  2.52278197],[-1.31386475,  0.88462238],[-2.11868196,  4.70957306],[-2.94996636,  2.59532259],\n[-3.54535995,  1.45352268],[ 0.98236743, -1.10106763],[-1.18504653, -0.2056499 ],[-1.51385164,  3.23671627],\n[-4.02378514,  2.2870068 ],[ 0.62524497, -0.16051336],[-3.76883635,  2.76996928],[ 0.74505627,  1.97611078],[-1.24412333, -0.62641691],[-0.80376609, -2.41908317],[-0.92379202, -1.02387576],[ 1.12397796, -0.13191423]])\ny = np.array([-1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1, -1])\n\n# Set the two coefficients and the intercept to various values and observe the resulting decision boundaries.\n# Set the coefficients\nmodel.coef_ = np.array([[-1,1]])\nmodel.intercept_ = np.array([-3])\n\n# Plot the data and decision boundary\n#plot_classifier(X,y,model)\n\n# Print the number of errors\nnum_err = np.sum(y != model.predict(X))\nprint(\"Number of errors:\", num_err)"},"cell_type":"code","id":"c35eff4f-0231-4273-af12-cae37069b9fe","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mintercept_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot the data and decision boundary\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#plot_classifier(X,y,model)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the number of errors\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m num_err \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y \u001b[38;5;241m!=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_err)\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:452\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     indices \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m[indices]\n","\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'classes_'"],"ename":"AttributeError","evalue":"'LogisticRegression' object has no attribute 'classes_'"}]},{"source":"```python\n# Set the coefficients\nmodel.coef_ = np.array([[-1,1]])\nmodel.intercept_ = np.array([-3])\n\n# Plot the data and decision boundary\nplot_classifier(X,y,model)\n\n# Print the number of errors\nnum_err = np.sum(y != model.predict(X))\nprint(\"Number of errors:\", num_err)\nNumber of errors: 0\n```","metadata":{},"cell_type":"markdown","id":"52a9b65d-41ee-487a-adb9-5a9e15ac8955"},{"source":"### Minimizing a loss function\nIn this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n\nWe'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model.\n\n```python\n# loss is the square of the difference between the true and predicted y-values (because we want them to be similar).\n# The squared error, summed over training examples\ndef my_loss(w):\n    s = 0\n    for i in range(y.size):\n        # Get the true and predicted target values for example 'i'\n        y_i_true = y[i]\n        y_i_pred = w@X[i]\n        s = s + (y_i_true - y_i_pred)**2\n    return s\n\n# Returns the w that makes my_loss(w) smallest\nw_fit = minimize(my_loss, X[0]).x\nprint(w_fit)\n\n# Compare with scikit-learn's LinearRegression coefficients\nlr = LinearRegression(fit_intercept=False).fit(X,y)\nprint(lr.coef_)\n\n[-9.28967297e-02  4.87153175e-02 -4.05723042e-03  2.85399119e+00\n -2.86835054e+00  5.92815219e+00 -7.26944750e-03 -9.68513678e-01\n  1.71156278e-01 -9.39664456e-03 -3.92187072e-01  1.49054687e-02\n -4.16304299e-01]\n\n[-9.28965170e-02  4.87149552e-02 -4.05997958e-03  2.85399882e+00\n -2.86843637e+00  5.92814778e+00 -7.26933458e-03 -9.68514157e-01\n  1.71151128e-01 -9.39621540e-03 -3.92190926e-01  1.49056102e-02\n -4.16304471e-01]\n```\n\n\n### Implementing logistic regression\nThis is very similar to the earlier exercise where you implemented linear regression \"from scratch\" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this in Chapter 3!).\n\nThe log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y.\n\n```python\n# The logistic loss, summed over training examples\ndef my_loss(w):\n    s = 0\n    for i in range(y.size):\n        raw_model_output = w@X[i]\n        s = s + log_loss(raw_model_output * y[i])\n    return s\n\n# Returns the w that makes my_loss(w) smallest\nw_fit = minimize(my_loss, X[0]).x\nprint(w_fit)\n\n# Compare with scikit-learn's LogisticRegression\nlr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\nprint(lr.coef_)\n```","metadata":{},"cell_type":"markdown","id":"091a7b14-664a-4e36-a606-09d53679094f"},{"source":"### Regularized logistic regression\nIn Chapter 1, you used logistic regression on the handwritten digits data set. Here, we'll explore the effect of L2 regularization.\n\nThe handwritten digits dataset is already loaded, split, and stored in the variables X_train, y_train, X_valid, and y_valid. The variables train_errs and valid_errs are already initialized as empty lists.\n\n```python\n# Train and validaton errors initialized as empty list\ntrain_errs = list()\nvalid_errs = list()\n\n# Loop over values of C_value\nfor C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n    # Create LogisticRegression object and fit\n    lr = LogisticRegression(C=C_value)\n    lr.fit(X_train, y_train)\n    \n    # Evaluate error rates and append to lists\n    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n    \n# Plot results\nplt.semilogx(C_values, train_errs, C_values, valid_errs)\nplt.legend((\"train\", \"validation\"))\nplt.show()\n```\n![Screen Shot 2023-03-21 at 12.09.42 PM](Screen%20Shot%202023-03-21%20at%2012.09.42%20PM.png)\n* As you can see, too much regularization `(small C)` doesn't work well - due to underfitting - and too little regularization (`large C`) doesn't work well either - due to overfitting.","metadata":{},"cell_type":"markdown","id":"414c070d-209b-4b8d-bccc-376a6ac69307"},{"source":"### Logistic regression and feature selection\nIn this exercise we'll perform `feature selection` on the movie review sentiment data set using `L1 regularization`. The features and targets are already loaded for you in X_train and y_train.\n\nWe'll search for the best value of C using scikit-learn's GridSearchCV(), which was covered in the prerequisite course.\n\n```python\n# Specify L1 regularization\nlr = LogisticRegression(solver='liblinear', penalty='l1')\n\n# Instantiate the GridSearchCV object and run the search : Find the value of C that minimizes cross-validation error\nsearcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters\nprint(\"Best CV params\", searcher.best_params_)\n\n# Find the number of nonzero coefficients (selected features)\nbest_lr = searcher.best_estimator_\ncoefs = best_lr.coef_\nprint(\"Total number of features:\", coefs.size)\nprint(\"Number of selected features:\", np.count_nonzero(coefs))\n\n<script.py> output:\n    Best CV params {'C': 1}\n    Total number of features: 2500\n    Number of selected features: 1219\n```\n\n### Identifying the most positive and negative words\nIn this exercise we'll try to interpret the coefficients of a logistic regression fit on the movie review sentiment dataset. The model object is already instantiated and fit for you in the variable lr.\n\nIn addition, the words corresponding to the different features are loaded into the variable vocab. For example, since vocab[100] is \"think\", that means feature 100 corresponds to the number of times the word \"think\" appeared in that movie review.\n\n```python\n<script.py> output:\n    Most positive words: favorite, superb, noir, knowing, excellent, \n    \n    Most negative words: worst, disappointing, waste, boring, lame, \n```","metadata":{},"cell_type":"markdown","id":"8305bb6a-410e-4804-8e8e-59318b98bf4f"},{"source":"### Regularization and probabilities\nIn this exercise, you will observe the effects of changing the regularization strength on the predicted probabilities.\n\nA 2D binary classification dataset is already loaded into the environment as X and y.\n\n```python\n# Set the regularization strength\nmodel = LogisticRegression(C=0.1)\n\n# Fit and plot\nmodel.fit(X,y)\nplot_classifier(X,y,model,proba=True)\n\n# Predict probabilities on training points\nprob = model.predict_proba(X)\nprint(\"Maximum predicted probability\", np.max(prob))\n\n<script.py> output:\n    Maximum predicted probability 0.9352061680350907\n```\n* As you probably noticed, smaller values of `C` lead to less confident predictions. That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function.\n\n\n### Visualizing easy and difficult examples\nIn this exercise, you'll visualize the examples that the logistic regression model is most and least confident about by looking at the largest and smallest predicted probabilities.\n\nThe handwritten digits dataset is already loaded into the variables X and y. The show_digit function takes in an integer index and plots the corresponding image, with some extra information displayed above the image.\n\n```python\nIn [1]:\nproba_inds\nOut[1]:\narray([1658, 1553,  363, ..., 1512, 1625,   32])\nIn [2]:\nproba.shape \nOut[2]:\n(1797, 10)\n\nlr = LogisticRegression()\nlr.fit(X,y)\n\n# Get predicted probabilities\nproba = lr.predict_proba(X)\n\n# Sort the example indices by their maximum probability\nproba_inds = np.argsort(np.max(proba,axis=1))\n\n# Show the most confident (least ambiguous) digit\nshow_digit(proba_inds[-1], lr)\n\n# Show the least confident (most ambiguous) digit\nshow_digit(proba_inds[0], lr)\n\nIn [7]:\nproba_inds[-1]\nOut[7]:\n32\nIn [8]:\nproba_inds[0]\nOut[8]:\n1658\nIn [9]:\nproba[32]\nOut[9]:\n\narray([2.18516077e-19, 1.56885283e-16, 6.58979958e-25, 1.24778742e-14,\n       2.47406822e-18, 1.00000000e+00, 1.39011478e-20, 9.52234164e-17,\n       2.33338796e-20, 4.28560936e-15])\nIn [10]:\nproba[1658]\nOut[10]:\n\narray([9.53629594e-08, 3.38958427e-03, 2.64586443e-07, 5.19300911e-02,\n       2.22432835e-14, 2.45772528e-04, 6.91539273e-10, 1.95721546e-09,\n       1.37667628e-01, 8.06766561e-01])\n```","metadata":{},"cell_type":"markdown","id":"bd77d2c3-ed8b-49c6-80b5-e6ad026592a2"},{"source":"### Fitting multi-class logistic regression\nIn this exercise, you'll fit the two types of multi-class logistic regression, one-vs-rest and softmax/multinomial, on the handwritten digits data set and compare the results. The handwritten digits dataset is already loaded and split into X_train, y_train, X_test, and y_test.\n\n```python\n# Fit a one-vs-rest logistic regression classifier by setting the multi_class parameter and report the results.\nlr_ovr = LogisticRegression(multi_class='ovr')\nlr_ovr.fit(X_train, y_train)\n\nprint(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\nprint(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n\n# Fit a multinomial logistic regression classifier by setting the multi_class parameter and report the results.\nlr_mn = LogisticRegression(multi_class='multinomial')\nlr_mn.fit(X_train, y_train)\n\nprint(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\nprint(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))\n\nOVR training accuracy: 0.9955456570155902\nOVR test accuracy    : 0.9644444444444444\nSoftmax training accuracy: 1.0\nSoftmax test accuracy    : 0.9688888888888889\n```","metadata":{},"cell_type":"markdown","id":"fad48486-93c0-44f4-813e-dcddd8e7c0f6"},{"source":"### Visualizing multi-class logistic regression\nIn this exercise we'll continue with the two types of multi-class logistic regression, but on a toy 2D data set specifically designed to break the one-vs-rest scheme.\n\nThe data set is loaded into X_train and y_train. The two logistic regression objects,lr_mn and lr_ovr, are already instantiated (with C=100), fit, and plotted.\n\nNotice that lr_ovr never predicts the dark blue classâ€¦ yikes! Let's explore why this happens by plotting one of the binary classifiers that it's using behind the scenes.\n\n```python\n# Print training accuracies\nprint(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\nprint(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n\n# Create the binary classifier (class 1 vs. rest)\nlr_class_1 = LogisticRegression(C=100)\nlr_class_1.fit(X_train, y_train==1)\n\n# Plot the binary classifier (class 1 vs. rest)\nplot_classifier(X_train, y_train==1, lr_class_1)\n```","metadata":{},"cell_type":"markdown","id":"dec2811c-b73f-457c-b789-dee7e5524eca"},{"source":"## Support Vectors\n* There are two ways to become a support vector: \n    * either the point is classified incorrectly, \n    * or it is classified correctly but very close to the boundary.\n    * **All** incorrectly classified points are support vectors.\n\n\n### Effect of removing examples\nSupport vectors are defined as training examples that influence the decision boundary. In this exercise, you'll observe this behavior by removing non support vectors from the training set.\n\nThe wine quality dataset is already loaded into X and y (first two features only). (Note: we specify lims in plot_classifier() so that the two plots are forced to use the same axis limits and can be compared directly.)\n\n```python\n# Train a linear SVM\nsvm = SVC(kernel=\"linear\")\nsvm.fit(X, y)\nplot_classifier(X, y, svm, lims=(11,15,0,6))\n\n# Make a new data set keeping only the support vectors\nprint(\"Number of original examples\", len(X))\nprint(\"Number of support vectors\", len(svm.support_))\nX_small = X[svm.support_]\ny_small = y[svm.support_]\n\n# Train a new SVM using only the support vectors\nsvm_small = SVC(kernel=\"linear\")\nsvm_small.fit(X_small, y_small)\nplot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))\n```\n\n### GridSearchCV warm-up\nIn the video we saw that increasing the RBF kernel hyperparameter gamma increases training accuracy. In this exercise we'll search for the gamma that maximizes cross-validation accuracy using scikit-learn's GridSearchCV. A binary version of the handwritten digits dataset, in which you're just trying to predict whether or not an image is a \"2\", is already loaded into the variables X and y.\n\n```python\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, parameters)\nsearcher.fit(X, y)\n\n# Report the best parameters\nprint(\"Best CV params\", searcher.best_params_) # Best CV params {'gamma': 0.001}\n```\n\n### Jointly tuning gamma and C with GridSearchCV\nIn the previous exercise the best value of gamma was 0.001 using the default value of C, which is 1. In this exercise you'll search for the best combination of C and gamma using GridSearchCV.\n\nAs in the previous exercise, the 2-vs-not-2 digits dataset is already loaded, but this time it's split into the variables X_train, y_train, X_test, and y_test. Even though cross-validation already splits the training set into parts, it's often a good idea to hold out a separate test set to make sure the cross-validation results are sensible\n\n```python\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, parameters)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n\nBest CV params {'C': 1, 'gamma': 0.001}\nBest CV accuracy 0.9988826815642458\nTest accuracy of best grid search hypers: 0.9988876529477196\n```\n\n### Using SGDClassifier\nIn this final coding exercise, you'll do a hyperparameter search over the regularization strength and the loss (logistic regression vs. linear SVM) using SGDClassifier().\n\n```python\n# We set random_state=0 for reproducibility \nlinear_classifier = SGDClassifier(random_state=0)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge', 'log_loss']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n\n<script.py> output:\n    Best CV params {'alpha': 0.001, 'loss': 'hinge'}\n    Best CV accuracy 0.9490730158730158\n    Test accuracy of best grid search hypers: 0.9611111111111111\n```\n* One advantage of SGDClassifier is that it's very fast - this would have taken a lot longer with LogisticRegression or LinearSVC.","metadata":{},"cell_type":"markdown","id":"c9f5e868-f0bb-4a5d-ab5d-4346bee8d919"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}