{"cells":[{"source":"# Machine learning with scikit-learn\n\n<br>\n\n## Supervised Learning with scikit-learn\n* Requirements to Proceed\n    * No Missing Values\n    * Data in numeric format\n    * Data stored in DataFrame or NumPy array \n\n\n* `Supervised learning` is a type of machine learning where the values to be predicted are already known, and a model is built with the aim of accurately predicting values of previously unseen data. \n* `Supervised learning` uses **features** to predict the value of a `target` variable, such as predicting a basketball player's position based on their points per game.\n\n\n## Classification Challenge\n* Classifying labels of unseen data\n1. Build a Model\n2. Model learns from the labeled data we pass to it\n3. Pass unlabeled data to the model as input\n4. Model predicts the labels of the unseen data\n\n* `Labeled Data` = training data\n\n###  k-Nearest Neightbors\n* The idea of k-Nearest Neighbors, or `KNN`, is to **predict the label of any data point by looking at the k**, for example, three, closest labeled data points and getting them to vote on what label the unlabeled observation should have. \n* `KNN` uses `majority voting`, which makes predictions based on what label the majority of nearest neighbors have.\n* `KNN` stands for K-nearest neighbour, it’s one of the Supervised learning algorithm mostly used for classification of data on the basis how it’s neighbour are classified. \n    * KNN stores all available cases and classifies new cases based on a similarity measure. \n    * **K** in KNN is a parameter that refers to the `number of the nearest neighbours to include in the majority voting process`.\n    * neighbors argument from sklearn\n* https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html","metadata":{},"cell_type":"markdown","id":"c20bbd9d-11dd-4aaa-b6d6-65f8de132f0b"},{"source":"```python\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Can't find this publicly so simply using the frame from the exercise\nchurn_df = pd.read_csv('datasets/churn.csv')\n\n# Create arrays for the features and target variables (use values property on pandas series for numpy array return)\ny = churn_df['churn'].values\nX = churn_df[['account_length', 'customer_service_calls']].values\n\nprint(y[:5], '\\n\\n', X[:5])\n\n# Create a KNN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X, y)\n\n# Output (X is a )\nKNeighborsClassifier(n_neighbors=6)\n\n<script.py> output:\n    [1 0 0 0 0] \n    \n     [[101   3]\n     [ 73   2]\n     [ 86   4]\n     [ 59   1]\n     [129   1]\n\n# X 2D numpy array quick peek with the features\n      \nX.shape\nOut[3]:\n(3333, 2)\n\nIn [2]:\nchurn_df[['account_length', 'customer_service_calls']].head()\nOut[2]:\n\n   account_length  customer_service_calls\n0             101                       3\n1              73                       2\n2              86                       4\n3              59                       1\n4             129                       1\n```\n\n### Predict\nk-Nearest Neighbors: Predict\nNow you have fit a KNN classifier, you can use it to predict the label of new data points. All available data was used for training, however, fortunately, there are new observations available. These have been preloaded for you as X_new.\n\nThe model knn, which you created and fit the data in the last exercise, has been preloaded for you. You will use your classifier to predict the labels of a set of new data points:\n\n```python\n# X_new are new observations for the features above in the same shape\nX_new = np.array([[30.0, 17.5],\n                  [107.0, 24.1],\n                  [213.0, 10.9]])\n\n# Predict the labels for the X_new\ny_pred = knn.predict(X_new)\n\n# Print the predictions for X_new\nprint(\"Predictions: {}\".format(y_pred)) \n\n# Output\nPredictions: [0 1 0]\n```","metadata":{},"cell_type":"markdown","id":"493b626e-5b1a-498e-9061-d601481b8911"},{"source":"### Measuring model performance \n\n**Computing Accuracy**\n1. Split Data into Training & Testing Set\n2. Fit/train classifier on training set\n3. Calculate accuracy using test set\n\n### Train/test split\n```python\nfrom sklearn.model_selection import train_test_split\n# to method pass features, target \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n\nknn = kNeighborsClassifers(n=6)\nknn.fit(X_train, y_train)\nprint(knn.score(X_test, y_test))\n# 0.88 rounded score\n```\n\n#### Flow\nTo do this, we import train_test_split from sklearn-dot-model_selection. \n* We call train_test_split, passing our features and targets. We commonly use 20-30% of our data as the test set. By setting the test_size argument to zero-point-three we use 30% here. \n* The random_state argument sets a seed for a random number generator that splits the data. \n    * Using the same number when repeating this step allows us to reproduce the exact split and our downstream results. \n    * It is best practice to ensure our split reflects the proportion of labels in our data. So if churn occurs in 10% of observations, we want 10% of labels in our training and test sets to represent churn. \n* We achieve this by setting stratify equal to y. \n* train_test_split returns four arrays: \n    * the training data, \n    * the test data, \n    * the training labels, \n    * and the test labels. \n    * We unpack these into `X_train, X_test, y_train, and y_test`, respectively. \n* We then instantiate a KNN model and fit it to the training data using the dot-fit method.\n*  To check the accuracy, we use the dot-score method, passing X test and y test. \n*  The accuracy of our model is 88%, which is low given our labels have a 9 to 1 ratio.","metadata":{},"cell_type":"markdown","id":"66d9e13a-44eb-45f6-a75d-d94571054603"},{"source":"### Exercise : Compute Accuracy\n* Split X and y into training and test sets, setting test_size equal to 20%, random_state to 42, and ensuring the target label proportions reflect that of the original dataset.\n\n```python\n# Import the module\nfrom sklearn.model_selection import train_test_split\n\n# Features X is from the churn_df with the target column dropped\nX = churn_df.drop(\"churn\", axis=1).values\ny = churn_df[\"churn\"].values\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))\n\n0.8740629685157422\n```\n\n### Overfitting and underfitting\nInterpreting model complexity is a great way to evaluate performance when utilizing supervised learning. Your aim is to produce a model that can interpret the relationship between features and the target variable, as well as generalize well when exposed to new observations.\n\nYou will generate accuracy scores for the training and test sets using a KNN classifier with different n_neighbor values, which you will plot in the next exercise.\n\n```python\n# Create neighbors as a numpy array of values from 1 up to and including 12\nneighbors = np.arange(1, 13)\ntrain_accuracies = {}\ntest_accuracies = {}\n\nfor neighbor in neighbors:\n  \n\t# Set up a KNN Classifier\n\tknn = KNeighborsClassifier(n_neighbors=neighbor)\n  \n\t# Fit the model\n\tknn.fit(X_train, y_train)\n  \n\t# Compute accuracy\n\ttrain_accuracies[neighbor] = knn.score(X_train, y_train)\n\ttest_accuracies[neighbor] = knn.score(X_test, y_test)\n\nprint(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n\n[ 1  2  3  4  5  6  7  8  9 10 11 12]\n# Train Accuracies \n {1: 1.0, 2: 0.887943971985993, 3: 0.9069534767383692, 4: 0.8734367183591796, 5: 0.8829414707353677, 6: 0.8689344672336168, 7: 0.8754377188594297, 8: 0.8659329664832416, 9: 0.8679339669834918, 10: 0.8629314657328664, 11: 0.864432216108054, 12: 0.8604302151075538} \n\n# Test Accuracies\n {1: 0.7871064467766117, 2: 0.8500749625187406, 3: 0.8425787106446777, 4: 0.856071964017991, 5: 0.8553223388305847, 6: 0.861319340329835, 7: 0.863568215892054, 8: 0.8605697151424287, 9: 0.8620689655172413, 10: 0.8598200899550225, 11: 0.8598200899550225, 12: 0.8590704647676162}\n```\n* Notice how training accuracy decreases as the number of neighbors initially gets larger, and vice versa for the testing accuracy\n\n\n### Visualizing model complexity\nNow you have calculated the accuracy of the KNN model on the training and test sets using various values of n_neighbors, you can create a model complexity curve to visualize how performance changes as the model becomes less complex!\n\nThe variables neighbors, train_accuracies, and test_accuracies, which you generated in the previous exercise, have all been preloaded for you. You will plot the results to aid in finding the optimal number of neighbors for your model.\n\n\n* Using the dictionaries above to plot the line visualtion for model accuracy for each set\n```python\n# Add a title\nplt.title(\"KNN: Varying Number of Neighbors\")\n\n# Plot training accuracies\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n\n# Plot test accuracies\nplt.plot(test_accuracies.keys(), test_accuracies.values(), label=\"Testing Accuracy\")\n\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\n\n# Display the plot\nplt.show()\n```\n![Screen Shot 2023-03-15 at 9.24.06 AM](Screen%20Shot%202023-03-15%20at%209.24.06%20AM.png)\n* See how training accuracy decreases and test accuracy increases as the number of neighbors gets larger. For the test set, accuracy peaks with 7 neighbors, suggesting it is the optimal value for our model.","metadata":{},"cell_type":"markdown","id":"bfe1c86b-970a-40ae-90e4-956a8f1cb42e"},{"source":"### Sklearn w/Regression\n* Now we're going to check out the other type of supervised learning: regression. In regression tasks, the target variable typically has continuous values, such as a country's GDP, or the price of a house.\n\n#### Sklearn Features Consideration\n* This is fine for y, but our **features** must be formatted as a two-dimensional array to be accepted by scikit-learn. To convert the shape of X_bmi we apply `NumPy's dot-reshape method`, passing minus one followed by one. Printing the shape again shows X_bmi is now the correct shape for our model.\n```python\n# Set bmi as single predictor feature value\n# slice out the BMI column of X, which is the fourth column, storing as the variable X_bmi\nX_bmi = X[:, 3]\nprint(X_bmi.shape) # (752,) 1Dimensional\n# Reshape for sklearn acceptance\nX_bmi = X_bmi.reshape(-1, 1)\nprint(X_bmi.shape) # (752,1) 2Dimensional feature for single variable \n```\n\n#### Exercise\n* In this chapter, you will work with a dataset called sales_df, which contains information on advertising campaign expenditure across different media types, and the number of dollars generated in sales for the respective campaign\n\n* You will use the advertising expenditure as features to predict sales values, initially working with the \"radio\" column.\n```python\nIn [2]:\nsales_df.head()\nOut[2]:\n\n        tv     radio  social_media      sales\n0  16000.0   6566.23       2907.98   54732.76\n1  13000.0   9237.76       2409.57   46677.90\n2  41000.0  15886.45       2913.41  150177.83\n3  83000.0  30020.03       6922.30  298246.34\n4  15000.0   8437.41       1406.00   56594.18\n\nIn [3]:\nsales_df.isna().sum()\nOut[3]:\n\ntv              0\nradio           0\nsocial_media    0\nsales           0\ndtype: int64\n# No null values to consider\n\n```\n```python\nimport numpy as np\n\n# Create X from the radio column's values (sales_df) advertising campaign expenditure\nX = sales_df['radio'].values\n\n# Create y from the sales column's values (sales_df) `sales` target predictor\ny = sales_df['sales'].values\n\n# Reshape X\nX = X.reshape(-1, 1)\n\n# Check the shape of the features and targets\nprint(X.shape, '\\n', y.shape)\n\n<script.py> output:\n    (4546, 1) \n     (4546,)\n```\n\n* Building a linear regression model\nNow you have created your feature and target arrays, you will train a linear regression model on all feature and target values.\n```python\n# Import LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create the model\nreg = LinearRegression()\n\n# Fit the model to the data\nreg.fit(X, y)\n\n# Make predictions\npredictions = reg.predict(X)\n\nprint(predictions[:5])\n\n[ 95491.17119147 117829.51038393 173423.38071499 291603.11444202\n 111137.28167129]\n```\n\n* Visualizing Linear Regression Model\n    * The variables X, an array of radio values, y, an array of sales values, and predictions, an array of the model's predicted values for y given X, have all been preloaded for you from the previous exercise.   \n```python\n# Import matplotlib.pyplot\nimport matplotlib.pyplot as plt\n\n# Create scatter plot\nplt.scatter(X, y, color=\"blue\")\n\n# Create line plot\nplt.plot(X, predictions, color=\"red\")\nplt.xlabel(\"Radio Expenditure ($)\")\nplt.ylabel(\"Sales ($)\")\n\n# Display the plot\nplt.show()\n```\n![Screen Shot 2023-03-15 at 9.57.58 AM](Screen%20Shot%202023-03-15%20at%209.57.58%20AM.png)\n","metadata":{},"cell_type":"markdown","id":"e4a626e5-accf-4bf5-ac9c-ba1438e1ceec"},{"source":"### Fit and predict for regression\nNow you have seen how linear regression works, your task is to create a multiple linear regression model using all of the features in the sales_df dataset, which has been preloaded for you\n```python\n# Create X and y arrays\nX = sales_df.drop(\"sales\", axis=1).values\ny = sales_df[\"sales\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Instantiate the model\nreg = LinearRegression()\n\n# Fit the model to the data\nreg.fit(X_train, y_train)\n\n# Make predictions\ny_pred = reg.predict(X_test)\nprint(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))\n\n<script.py> output:\n    Predictions: [53176.66154234 70996.19873235], Actual Values: [55261.28 67574.9 ]\n    \nThe first two predictions appear to be within around 5% of the actual values from the test set!\n```\n\n### Regression performance\nNow you have fit a model, reg, using all features from sales_df, and made predictions of sales values, you can evaluate performance using some common regression metrics.\n\n* Your task is to find out how well the features can explain the variance in the target values, along with assessing the model's ability to make predictions on unseen data.\n```python\n# Import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\n\n# Compute R-squared - model.score : Return the coefficient of determination of the prediction.\nr_squared = reg.score(X_test, y_test)\n\n# Compute RMSE : the average squared difference between the estimated values and the actual value\nrmse = mean_squared_error(y_test, y_pred, squared=False)\n\n# Print the metrics\nprint(\"R^2: {}\".format(r_squared))\nprint(\"RMSE: {}\".format(rmse))\n\nR^2: 0.9990165886162027\nRMSE: 2942.372219812037\n```\n* the features explain 99.9% of the variance in sales values! Looks like this company's advertising strategy is working well!","metadata":{},"cell_type":"markdown","id":"8afae2bf-0aea-41a0-8c19-d0eba803498c"},{"source":"## Cross-validation\n\nIf we're computing `R-squared` on our test set, the R-squared returned is dependent on the way that we split up the data! The data points in the test set may have some peculiarities that mean the R-squared computed on it is not representative of the model's ability to generalize to unseen data. To combat this dependence on what is essentially a random split, we use a technique called `cross-validation`.\n\nAs we split the dataset into five folds, we call this process 5-fold cross-validation. If we use 10 folds, it is called 10-fold cross-validation. More generally, if we use k folds, it is called k-fold cross-validation or k-fold CV. There is, however, a trade-off. Using more folds is more computationally expensive. This is because we are fitting and predicting more times.\n\n```python\nfrom sklearn.model_selection import cross_val_score, KFold\nkf = KFold(n_splits=6, shuffle=True, random_state=42)\nreg = LinearRegression\ncv_results = cross_val_score(reg, X, y, cv=kf)\n```\n\n### Cross-validation for R-squared : Ex\nCross-validation is a vital approach to evaluating a model. It maximizes the amount of data that is available to the model, as the model is not only trained but also tested on all of the available data.\n\nIn this exercise, you will build a linear regression model, then use 6-fold cross-validation to assess its accuracy for predicting sales using social media advertising expenditure. You will display the individual score for each of the six-folds.\n\n```python\n# Import the necessary modules\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Create a KFold object\nkf = KFold(n_splits=6, shuffle=True, random_state=5)\n\nreg = LinearRegression()\n\n# Compute 6-fold cross-validation scores\ncv_scores = cross_val_score(reg, X, y, cv=kf)\n\n# Print scores\nprint(cv_scores)\n\n[0.74451678 0.77241887 0.76842114 0.7410406  0.75170022 0.74406484]\n```\n* Notice how R-squared for each fold ranged between 0.74 and 0.77? By using cross-validation, we can see how performance varies depending on how the data is split\n\n\n### Analyzing cross-validation metrics\nNow you have performed cross-validation, it's time to analyze the results.\n\nYou will display the mean, standard deviation, and 95% confidence interval for cv_results, which has been preloaded for you from the previous exercise.\n\n```python\n# Print the mean\nprint(np.mean(cv_results))\n\n# Print the standard deviation\nprint(np.std(cv_results))\n\n# Print the 95% confidence interval\nprint(np.quantile(cv_results, [.025, .975]))\n\n0.7536937416666666\n0.012305386274436092\n[0.74141863 0.77191915]\n```","metadata":{},"cell_type":"markdown","id":"5a3e3e98-f540-4a0e-89e6-ad6bcacc0624"},{"source":"### Regularized Regression\n#### Why Regularize\n* Linear Regresssion : minimizes a loss function\n* The model/function of linreg chooses a coefficient, a, for each variable/feature, plus b\n* large coefficients can lead to `overfitting`\n* Regularization: Penalize large coefficients\n\n#### Ridge regression\n* Ridge penalizes large positive or negative coefficients\n* Be careful of alpha which controls model complexity\n    * low alpha can lead to overfitting\n    * high alpha can lead to underfitting\n\nTo highlight the impact of different alpha values, we create an empty list for our scores, then loop through a list of different alpha values. Inside the for loop we instantiate Ridge, setting the alpha keyword argument equal to the iterator, also called alpha. We fit on the training data, and predict on the test data. We save the model's R-squared value to the scores list. Finally, outside of the loop, we print the scores for the models with five different alpha values. We see performance gets worse as alpha increases.\n\n\n\n#### Exercises\nRegularized regression: Ridge\nRidge regression performs regularization by computing the squared values of the model parameters multiplied by alpha and adding them to the loss function.\n\nIn this exercise, you will fit ridge regression models over a range of different alpha values, and print their \n scores. You will use all of the features in the sales_df dataset to predict \"sales\". The data has been split into X_train, X_test, y_train, y_test for you.\n \n```python\n# Import Ridge\nfrom sklearn.linear_model import Ridge\nalphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\nridge_scores = []\nfor alpha in alphas:\n  \n  # Create a Ridge regression model\n  ridge = Ridge()\n  \n  # Fit the data\n  ridge.fit(X_train, y_train)\n  \n  # Obtain R-squared\n  score = ridge.score(X_test, y_test)\n  ridge_scores.append(score)\n  \nprint(ridge_scores)\n[0.9990152104759369, 0.9990152104759373, 0.9990152104759419, 0.9990152104759871, 0.9990152104764387, 0.9990152104809561]\n```\n* The scores don't appear to change much as alpha increases, which is indicative of how well the features explain the variance in the target—even by heavily penalizing large coefficients, underfitting does not occur!\n\n#### Lasso\n* Can be used to identify important features in a dataset\n```python\n# Import Lasso\nfrom sklearn.linear_model import Lasso\n\n# Instantiate a lasso regression model\nlasso = Lasso(alpha=0.3)\n\n# Fit the model to the data\nlasso.fit(X, y)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_\nprint(lasso_coef)\n[ 3.56256962 -0.00397035  0.00496385]\nplt.bar(sales_columns, lasso_coef)\nplt.xticks(rotation=45)\nplt.show()\n```","metadata":{},"cell_type":"markdown","id":"0e507604-5572-4820-b72a-3bea8dd9cfa3"},{"source":"<br>\n\n## Fine-Tuning Your Model\nHaving trained models, now you will learn how to evaluate them. In this chapter, you will be introduced to several metrics along with a visualization technique for analyzing classification model performance using scikit-learn. You will also learn how to optimize classification and regression models through the use of hyperparameter tuning.\n\n### Deciding on a primary metric\nAs you have seen, several metrics can be useful to evaluate the performance of classification models, including accuracy, precision, recall, and F1-score.\n\n\n#### Assessing a diabetes prediction classifier\nIn this chapter you'll work with the diabetes_df dataset introduced previously.\n\nThe goal is to predict whether or not each individual is likely to have diabetes based on the features body mass index (BMI) and age (in years). Therefore, it is a binary classification problem. A target value of 0 indicates that the individual does not have diabetes, while a value of 1 indicates that the individual does have diabetes.\n\ndiabetes_df has been preloaded for you as a pandas DataFrame and split into X_train, X_test, y_train, and y_test. In addition, a KNeighborsClassifier() has been instantiated and assigned to knn.\n\nYou will fit the model, make predictions on the test set, then produce a confusion matrix and classification report.\n\n\n```python\n# Import confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the model to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[116  35]\n [ 46  34]]\n              precision    recall  f1-score   support\n\n           0       0.72      0.77      0.74       151\n           1       0.49      0.42      0.46        80\n\n    accuracy                           0.65       231\n   macro avg       0.60      0.60      0.60       231\nweighted avg       0.64      0.65      0.64       231\n```\n* The model produced 34 true positives and 35 false positives, meaning precision was less than 50%, which is confirmed in the classification report. The output also shows a better F1-score for the zero class, which represents individuals who do not have diabetes.\n\n\n### Logistic Regression & ROC\n\n####\nBuilding a logistic regression model\nIn this exercise, you will build a logistic regression model using all features in the diabetes_df dataset. The model will be used to predict the probability of individuals in the test set having a diabetes diagnosis.\n\nThe diabetes_df dataset has been split into X_train, X_test, y_train, and y_test, and preloaded for you.\n\n```python\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate the model\nlogreg = LogisticRegression()\n\n# Fit the model\nlogreg.fit(X_train, y_train)\n\n# probability estimates\nprint(logreg.predict_proba(X_test)[:5])\n\n# Predict probabilities : each individual in the test set having a diabetes diagnosis\n# Recall the second column of the results which contains all positive probabilities\ny_pred_probs = logreg.predict_proba(X_test)[:, 1]\n\nprint(y_pred_probs[:10])\n\n<script.py> output:\n    [[0.73448969 0.26551031]\n     [0.81663458 0.18336542]\n     [0.87880404 0.12119596]\n     [0.84386435 0.15613565]\n     [0.50388715 0.49611285]]\n    [0.26551031 0.18336542 0.12119596 0.15613565 0.49611285 0.44582236\n     0.01359235 0.61646125 0.55640546 0.7931187 ]\n```\n\n### The ROC curve\nNow you have built a logistic regression model for predicting diabetes status, you can plot the ROC curve to visualize how the true positive rate and false positive rate vary as the decision threshold changes.\n\n```python\nIn [2]:\ny_pred_probs[:5]\nOut[2]:\narray([0.26551031, 0.18336542, 0.12119596, 0.15613565, 0.49611285])\n# above positive probablities \n# Import roc_curve\nfrom sklearn.metrics import roc_curve\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n\nplt.plot([0, 1], [0, 1], 'k--')\n\n# Plot tpr against fpr\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Diabetes Prediction')\nplt.show()\n```\n* The ROC curve is above the dotted line, so the model performs better than randomly guessing the class of each observation.\n\n\n### ROC AUC\nThe ROC curve you plotted in the last exercise looked promising.\n\nNow you will compute the area under the ROC curve, along with the other classification metrics you have used previously.\n\nThe confusion_matrix and classification_report functions have been preloaded for you, along with the logreg model you previously built, plus X_train, X_test, y_train, y_test. Also, the model's predicted test set labels are stored as y_pred, and probabilities of test set observations belonging to the positive class stored as y_pred_probs.\n\nA knn model has also been created and the performance metrics printed in the console, so you can compare the roc_auc_score, confusion_matrix, and classification_report between the two models.\n\n```python\n# Import roc_auc_score\nfrom sklearn.metrics import roc_auc_score\n\n# Calculate roc_auc_score\nprint(roc_auc_score(y_test, y_pred_probs))\n\n# Calculate the confusion matrix\nprint(confusion_matrix(y_test, y_pred))\n\n# Calculate the classification report\nprint(classification_report(y_test, y_pred))\n\n<script.py> output:\n    0.8002483443708608\n    [[121  30]\n     [ 30  50]]\n                  precision    recall  f1-score   support\n    \n               0       0.80      0.80      0.80       151\n               1       0.62      0.62      0.62        80\n    \n        accuracy                           0.74       231\n       macro avg       0.71      0.71      0.71       231\n    weighted avg       0.74      0.74      0.74       231\n    \n```\n* Did you notice that logistic regression performs better than the KNN model across all the metrics you calculated? A ROC AUC score of 0.8002 means this model is 60% better than a chance model at correctly predicting labels! scikit-learn makes it easy to produce several classification metrics with only a few lines of code.","metadata":{},"cell_type":"markdown","id":"c57d8ddc-4c0c-46cf-a719-41ddbad762c5"},{"source":"## Hyperparameters\n\n### Hyperparameter tuning with GridSearchCV\nNow you have seen how to perform grid search hyperparameter tuning, you are going to build a lasso regression model with optimal hyperparameters to predict blood glucose levels using the features in the diabetes_df dataset.\n\n```python\n# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the parameter grid\nparam_grid = {\"alpha\": np.linspace(.00001, 1, 20)}\n\n# Instantiate lasso_cv\nlasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n\n# Fit to the training data\nlasso_cv.fit(X_train, y_train)\nprint(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\n# Scientific Notation unpacking\nprint(\"Tuned lasso paramaters: {:.5f}\".format(list(lasso_cv.best_params_.values())[0]))\nprint(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))\n\nTuned lasso paramaters: {'alpha': 1e-05}\nTuned lasso paramaters: 0.00001\nTuned lasso score: 0.33078807238121977\n```\n*  Unfortunately, the best model only has an R-squared score of 0.33, highlighting that using the optimal hyperparameters does not guarantee a high performing model!\n\n### Hyperparameter tuning with RandomizedSearchCV\nAs you saw, GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space. In this case, you can use `RandomizedSearchCV`, which tests a fixed number of hyperparameter settings from specified probability distributions.\n```python\n# Create the parameter space\nparams = {\"penalty\": [\"l1\", \"l2\"],\n         \"tol\": np.linspace(0.0001, 1.0, 50),\n         \"C\": np.linspace(.1, 1.0, 50),\n         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n\n# Instantiate the RandomizedSearchCV object\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n\n# Fit the data to the model\nlogreg_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n\n<script.py> output:\n    Tuned Logistic Regression Parameters: {'tol': 0.14294285714285712, 'penalty': 'l2', 'class_weight': 'balanced', 'C': 0.6326530612244898}\n    Tuned Logistic Regression Best Accuracy Score: 0.7460082633613221\n```\n","metadata":{},"cell_type":"markdown","id":"9f17d45b-9e24-4863-9cf0-0fa8e26a5d49"},{"source":"## Preprocessing and Pipelines\nLearn how to impute missing values, convert categorical data to numeric values, scale data, evaluate multiple supervised learning models simultaneously, and build pipelines to streamline your workflow!\n\n### Creating Dummy Variables\nBeing able to include categorical features in the model building process can enhance performance as they may add information that contributes to prediction accuracy.\n\nThe music_df dataset has been preloaded for you, and its shape is printed. Also, pandas has been imported as pd\n```python\nmusic_df.head()\nOut[1]:\n\n   popularity  acousticness  danceability  duration_ms  energy  ...  loudness  speechiness    tempo  valence       genre\n0        41.0         0.644         0.823     236533.0   0.814  ...    -5.611        0.177  102.619    0.649        Jazz\n1        62.0         0.086         0.686     154373.0   0.670  ...    -7.626        0.225  173.915    0.636         Rap\n2        42.0         0.239         0.669     217778.0   0.736  ...    -3.223        0.060  145.061    0.494  Electronic\n3        64.0         0.013         0.522     245960.0   0.923  ...    -4.560        0.054  120.406    0.595        Rock\n4        60.0         0.121         0.780     229400.0   0.467  ...    -6.645        0.253   96.056    0.312         Rap\n```\n* First string type column (non float) was designated in the music_df\n```python\n# Create music_dummies\nmusic_dummies = pd.get_dummies(music_df, drop_first=True)\n\n# Print the new DataFrame's shape\nprint(\"Shape of music_dummies: {}\".format(music_dummies.shape))\nShape of music_dummies: (1000, 20)\n```\n\n### Regression with categorical features\nNow you have created music_dummies, containing binary features for each song's genre, it's time to build a ridge regression model to predict song popularity.\n\nmusic_dummies has been preloaded for you, along with Ridge, cross_val_score, numpy as np, and a KFold object stored as kf.\n\nThe model will be evaluated by calculating the average RMSE, but first, you will need to convert the scores for each fold to positive values and take their square root. This metric shows the average error of our model's predictions, so it can be compared against the standard deviation of the target value—\"popularity\".\n\n```python\n# Create X and y\nX = music_dummies.drop('popularity', axis=1).values\ny = music_dummies['popularity'].values\n\n# Instantiate a ridge model\nridge = Ridge(alpha=0.2)\n\n# Perform cross-validation\nscores = cross_val_score(ridge, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n\n# Calculate RMSE\nrmse = np.sqrt(-scores)\nprint(\"Average RMSE: {}\".format(np.mean(rmse)))\nprint(\"Standard Deviation of the target array: {}\".format(np.std(y)))\n\n<script.py> output:\n    Average RMSE: 8.236853840202299\n    Standard Deviation of the target array: 14.02156909907019\n```\n\n### Handling Missing Data\n```python\n# Print missing values for each column\nprint(music_df.isna().sum().sort_values())\n\n# Remove values where less than 5% are missing\nmusic_df = music_df.dropna(subset=[\"genre\", \"popularity\", \"loudness\", \"liveness\", \"tempo\"])\n\n# Convert genre to a binary feature\nmusic_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n\nprint(music_df.isna().sum().sort_values())\nprint(\"Shape of the `music_df`: {}\".format(music_df.shape))\n\nscript.py> output:\n    genre                 8\n    popularity           31\n    loudness             44\n    liveness             46\n    tempo                46\n    speechiness          59\n    duration_ms          91\n    instrumentalness     91\n    danceability        143\n    valence             143\n    acousticness        200\n    energy              200\n    dtype: int64\n    popularity            0\n    liveness              0\n    loudness              0\n    tempo                 0\n    genre                 0\n    duration_ms          29\n    instrumentalness     29\n    speechiness          53\n    danceability        127\n    valence             127\n    acousticness        178\n    energy              178\n    dtype: int64\n    Shape of the `music_df`: (892, 12)\n```\n\n### Pipeline for song genre prediction: I (Impute Pipeline)\nNow it's time to build a pipeline. It will contain steps to impute missing values using the mean for each feature and build a KNN model for the classification of song genre.\n\nThe modified music_df dataset that you created in the previous exercise has been preloaded for you, along with KNeighborsClassifier and train_test_split.\n\n```python\n# Import modules\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n# Instantiate an imputer\nimputer = SimpleImputer()\n\n# Instantiate a knn model\nknn = KNeighborsClassifier(3)\n\n# Build steps for the pipeline\nsteps = [(\"imputer\", imputer), \n         (\"knn\", knn)]\n```\n\n### Pipeline for song genre prediction: II\nHaving set up the steps of the pipeline in the previous exercise, you will now use it on the music_df dataset to classify the genre of songs. What makes pipelines so incredibly useful is the simple interface that they provide.\n\nX_train, X_test, y_train, and y_test have been preloaded for you, and confusion_matrix has been imported from sklearn.metrics.\n\n```python\nsteps = [(\"imputer\", imp_mean),\n        (\"knn\", knn)]\n\n# Create the pipeline\npipeline = Pipeline(steps)\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Print the confusion matrix\nprint(confusion_matrix(y_test, y_pred))\n\n<script.py> output:\n    [[79  9]\n     [ 4 82]]\n```\n*  In this case, the confusion matrix highlights that the model had 79 true positives and 82 true negatives!\n\n\n## Centering and scaling\n* Many machine learning models use some form of distance to inform them, so if we have features on far larger scales, they can disproportionately influence our model. For example, KNN uses distance explicitly when making predictions. For this reason, we actually want features to be on a similar scale. To achieve this, we can normalize or standardize our data, often referred to as scaling and centering.\n\n### Centering and scaling for regression\nNow you have seen the benefits of scaling your data, you will use a pipeline to preprocess the music_df features and build a lasso regression model to predict a song's loudness.\n\n```python\n# Import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Create pipeline steps\nsteps = [(\"scaler\", StandardScaler()),\n         (\"lasso\", Lasso(alpha=0.5))]\n\n# Instantiate the pipeline\npipeline = Pipeline(steps)\npipeline.fit(X_train, y_train)\n\n# Calculate and print R-squared\nprint(pipeline.score(X_test, y_test))\n\n<script.py> output:\n    0.6193523316282489\n```\n### Centering and scaling for classification\nNow you will bring together scaling and model building into a pipeline for cross-validation.\n\nYour task is to build a pipeline to scale features in the music_df dataset and perform grid search cross-validation using a logistic regression model with different values for the hyperparameter C. The target variable here is \"genre\", which contains binary values for rock as 1 and any other genre as 0.\n\n```python\n# Build the steps\nsteps = [(\"scaler\", StandardScaler()),\n         (\"logreg\", LogisticRegression())]\npipeline = Pipeline(steps)\n\n# Create the parameter space\nparameters = {\"logreg__C\": np.linspace(0.001, 1.0, 20)}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=21)\n\n# Instantiate the grid search object\ncv = GridSearchCV(pipeline, param_grid=parameters)\n\n# Fit to the training data\ncv.fit(X_train, y_train)\nprint(cv.best_score_, \"\\n\", cv.best_params_)\n```\n\n### Visualizing Regression Model Performance\n```python\nmodels = {\"Linear Regression\": LinearRegression(), \"Ridge\": Ridge(alpha=0.1), \"Lasso\": Lasso(alpha=0.1)}\nresults = []\n\n# Loop through the models' values\nfor model in models.values():\n  kf = KFold(n_splits=6, random_state=42, shuffle=True)\n  \n  # Perform cross-validation (default model score) # Coefficient of determination R^2\n  cv_scores = cross_val_score(model, X_train, y_train, cv=kf)\n  \n  # Append the results\n  results.append(cv_scores)\n\nprint(len(results)) # 3 (Return of each r2)\n\nIn [1]:\nresults\nOut[1]:\n\n[array([0.71078444, 0.75604101, 0.80460283, 0.72571513, 0.72716622,\n        0.61911259]),\n array([0.71082272, 0.75603243, 0.8045894 , 0.72571954, 0.72715631,\n        0.61912813]),\n array([0.46884783, 0.4084498 , 0.41445316, 0.41556667, 0.40095602,\n        0.39870331])]\n\n# Create a box plot of the results\nplt.boxplot(results, labels=models.keys())\nplt.show()\n```\n![Screen Shot 2023-03-15 at 4.48.06 PM](Screen%20Shot%202023-03-15%20at%204.48.06%20PM.png)\n\n### Predicting on the test se\nIn the last exercise, `linear regression` and `ridge` appeared to produce similar results. It would be appropriate to select either of those models; however, you can check predictive performance on the test set to see if either one can outperform the other.\n\n```python\n# Import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\n\nfor name, model in models.items():\n  \n  # Fit the model to the training data\n  model.fit(X_train_scaled, y_train)\n  \n  # Make predictions on the test set\n  y_pred = model.predict(X_test_scaled)\n  \n  # Calculate the test_rmse\n  test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n  print(\"{} Test Set RMSE: {}\".format(name, test_rmse))\n\n<script.py> output:\n    Linear Regression Test Set RMSE: 0.11988851505947569\n    Ridge Test Set RMSE: 0.11987066103299668\n```\n\n### Visualizing classification model performance\nIn this exercise, you will be solving a classification problem where the \"popularity\" column in the music_df dataset has been converted to binary values, with 1 representing popularity more than or equal to the median for the \"popularity\" column, and 0 indicating popularity below the median.\n\nYour task is to build and visualize the results of three different models to classify whether a song is popular or not.\n\n```python\n# Create models dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree Classifier\": DecisionTreeClassifier()}\nresults = []\n\n# Loop through the models' values\nfor model in models.values():\n  \n  # Instantiate a KFold object\n  kf = KFold(n_splits=6, random_state=12, shuffle=True)\n  \n  # Perform cross-validation\n  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n  results.append(cv_results)\nplt.boxplot(results, labels=models.keys())\nplt.show()\n\nIn [1]:\nresults\nOut[1]:\n\n[array([0.8  , 0.752, 0.736, 0.8  , 0.736, 0.736]),\n array([0.696, 0.728, 0.728, 0.768, 0.72 , 0.72 ]),\n array([0.632, 0.696, 0.752, 0.728, 0.656, 0.648])]\n```\n![Screen Shot 2023-03-15 at 5.00.21 PM](Screen%20Shot%202023-03-15%20at%205.00.21%20PM.png)\n* Looks like logistic regression is the best candidate based on the cross-validation results!\n\n### Pipeline for predicting song popularity\nFor the final exercise, you will build a pipeline to impute missing values, scale features, and perform hyperparameter tuning of a logistic regression model. The aim is to find the best parameters and accuracy when predicting song genre!\n\n```python\n# Create steps\nsteps = [(\"imp_mean\", SimpleImputer()), \n         (\"scaler\", StandardScaler()), \n         (\"logreg\", LogisticRegression())]\n\n# Set up pipeline\npipeline = Pipeline(steps)\nparams = {\"logreg__solver\": [\"newton-cg\", \"saga\", \"lbfgs\"],\n         \"logreg__C\": np.linspace(0.001, 1.0, 10)}\n\n# Create the GridSearchCV object\ntuning = GridSearchCV(pipeline, param_grid=params)\ntuning.fit(X_train, y_train)\ny_pred = tuning.predict(X_test)\n\n# Compute and print performance :  best parameters and compute and print the test set accuracy score for the grid search object.\nprint(\"Tuned Logistic Regression Parameters: {}, Accuracy: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))\n\n<script.py> output:\n    Tuned Logistic Regression Parameters: {'logreg__C': 0.112, 'logreg__solver': 'newton-cg'}, Accuracy: 0.82\n```\n* you've selected a model, built a preprocessing pipeline, and performed hyperparameter tuning to create a model that is 82% accurate in predicting song genres!","metadata":{},"cell_type":"markdown","id":"4e92ebe0-4678-4cbf-957f-15d50e5e120b"},{"source":"![Screen Shot 2023-03-15 at 5.08.36 PM](Screen%20Shot%202023-03-15%20at%205.08.36%20PM.png)\n","metadata":{},"cell_type":"markdown","id":"e5409b69-abc3-4630-8429-25047d703bc5"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}